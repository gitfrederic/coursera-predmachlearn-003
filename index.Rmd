# Coursera - Practical Machine Learning Project

<br/>
<p/>

## Overview

A machine learning algorithm (random forest) was applied to predict how a specific physical activity was performed.  The 5 possible ways ('classe') that the activity could be performed is summarized in Table 1 below.

<cite>Table 1. Methods in which activity could be performed.</cite>
<table cellpadding="5" border="1">
    <tr>
        <th>Classe</th>
        <th>Description</th>
    </tr>
    <tr>
        <td>A</td>
        <td>Performed exactly according to the specification.</td>
    </tr>
    <tr>
        <td>B</td>
        <td>Performed by throwing the elbows to the front.</td>
    </tr>
    <tr>
        <td>C</td>
        <td>Performed by lifting the dumbbell only halfway.</td>
    </tr>
    <tr>
        <td>D</td>
        <td>Performed by lowering the dumbbell only halfway.</td>
    </tr>
    <tr>
        <td>E</td>
        <td>Performed by throwing the hips to the front .</td>
    </tr>
</table>

<p/>

The assignment asked that we predict which one of the 5 ways (A, B, C, D, or E) the activity was performed in the provided test dataset containing measurements from activity monitors.

<p/>

Using the approach described in this document, I was able to obtain a perfect score (20 / 20) on the submission part of the course project (although I suspect this isn't too difficult to achieve).

<br/>
<p/>


## How the algorithm was built

The first step in building the algorithm was to choose appropriate features from the provided training dataset:

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.*

The dataset is available at [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) in the "Weight Lifting Exercises Dataset" section.

The features were chosen on the basis that the majority of its entries contained valid (i.e. non-empty/non-NA) values and that they were meaningful for prediction purposes (e.g. where not time-based or task counters/identifiers).  In particular, the activity monitor measurements were considered more valuable than other measurements.  Here are some examples:

**Loading the raw training data set:**
```{r chunk1, message=FALSE, cache=TRUE, echo=TRUE}
trainingData <- read.table("data/pml-training.csv", header=TRUE, sep=c(','))
```

**A rejected feature:**
```{r chunk2, message=FALSE, echo=TRUE}
summary(trainingData$kurtosis_yaw_belt)
```

The rejected feature had many empty or invalid values (i.e. '#DIV/0!' values) which would not be very useful.  Similar features to this one were also exluded from consideration.

**An accepted feature:**
```{r chunk3, message=FALSE, echo=TRUE}
summary(trainingData$total_accel_belt)
```
In contrast to the reject feature, the above feature had no invalid values.  These types of features were kept for consideration.

After careful analysis of the training dataset, I found 52 features with which to build the model.  Table 2 below lists the 52 features used:

<cite>Table 2. The 52 features used to build the model.</cite>
<table cellpadding="5" border="1">
    <tr>
        <td>roll_belt</td>
        <td>pitch_belt</td>
        <td>yaw_belt</td>
        <td>total_accel_belt</td>
    </tr>
    <tr>
        <td>gyros_belt_x</td>
        <td>gyros_belt_y</td>
        <td>gyros_belt_z</td>
        <td>accel_belt_x</td>
    </tr>
    <tr>
        <td>accel_belt_y</td>
        <td>accel_belt_z</td>
        <td>magnet_belt_x</td>
        <td>magnet_belt_y</td>
    </tr>
    <tr>
        <td>magnet_belt_z</td>
        <td>roll_arm</td>
        <td>pitch_arm</td>
        <td>yaw_arm</td>
    </tr>
    <tr>
        <td>total_accel_arm</td>
        <td>gyros_arm_x</td>
        <td>gyros_arm_y</td>
        <td>gyros_arm_z</td>
    </tr>
    <tr>
        <td>accel_arm_x</td>
        <td>accel_arm_y</td>
        <td>accel_arm_z</td>
        <td>magnet_arm_x</td>
    </tr>
    <tr>
        <td>magnet_arm_y</td>
        <td>magnet_arm_z</td>
        <td>roll_dumbbell</td>
        <td>pitch_dumbbell</td>
    </tr>
    <tr>
        <td>yaw_dumbbell</td>
        <td>total_accel_dumbbell</td>
        <td>gyros_dumbbell_x</td>
         <td>gyros_dumbbell_y</td>
    </tr>
    <tr>
        <td>gyros_dumbbell_z</td>
        <td>accel_dumbbell_x</td>
        <td>accel_dumbbell_y</td>
        <td>accel_dumbbell_z</td>
    </tr>
    <tr>
        <td>magnet_dumbbell_x</td>
        <td>magnet_dumbbell_y</td>
        <td>magnet_dumbbell_z</td>
        <td>roll_forearm</td>
    </tr>
    <tr>
        <td>pitch_forearm</td>
        <td>yaw_forearm</td>
        <td>total_accel_forearm</td>
        <td>gyros_forearm_x</td>
    </tr>
    <tr>
        <td>gyros_forearm_y</td>
        <td>gyros_forearm_z</td>
        <td>accel_forearm_x</td>
        <td>accel_forearm_y</td>
    </tr>
    <tr>
        <td>accel_forearm_z</td>
        <td>magnet_forearm_x</td>
        <td>magnet_forearm_y</td>
        <td>magnet_forearm_z</td>
    </tr>
</table>

<p/>

I had initially included 'user_name' (as a factor), but I removed it since it would not make the model more generally useful.

<p/>

The R code below shows how the selected features were put together:

```{r chunk4, message=FALSE, cache=TRUE, echo=TRUE}

# Convert the 'classe' values to factors
trainingData$classe <- as.factor(trainingData$classe)

# Subset the training data (only relevant features used, see notes)
trainingData <- trainingData[c("classe",
                               "roll_belt",
                               "pitch_belt",
                               "yaw_belt", 
                               "total_accel_belt",
                               "gyros_belt_x",
                               "gyros_belt_y",
                               "gyros_belt_z",
                               "accel_belt_x",
                               "accel_belt_y",
                               "accel_belt_z",
                               "magnet_belt_x",
                               "magnet_belt_y",
                               "magnet_belt_z",
                               "roll_arm",
                               "pitch_arm",
                               "yaw_arm",
                               "total_accel_arm",
                               "gyros_arm_x",
                               "gyros_arm_y",
                               "gyros_arm_z",
                               "accel_arm_x",
                               "accel_arm_y",
                               "accel_arm_z",
                               "magnet_arm_x",
                               "magnet_arm_y",
                               "magnet_arm_z",
                               "roll_dumbbell",
                               "pitch_dumbbell",
                               "yaw_dumbbell",
                               "total_accel_dumbbell",
                               "gyros_dumbbell_x",
                               "gyros_dumbbell_y",
                               "gyros_dumbbell_z",
                               "accel_dumbbell_x",
                               "accel_dumbbell_y",
                               "accel_dumbbell_z",
                               "magnet_dumbbell_x",
                               "magnet_dumbbell_y",
                               "magnet_dumbbell_z",
                               "roll_forearm",
                               "pitch_forearm",
                               "yaw_forearm",
                               "total_accel_forearm",
                               "gyros_forearm_x",
                               "gyros_forearm_y",
                               "gyros_forearm_z",
                               "accel_forearm_x",
                               "accel_forearm_y",
                               "accel_forearm_z",
                               "magnet_forearm_x",
                               "magnet_forearm_y",
                               "magnet_forearm_z")]

```

<br/>
<p/>


Next, I looked at whether the training set outcome values are well-balanced:

```{r chunk5, echo=TRUE, message=FALSE}
table(trainingData$classe)
```

As you can see from the output above, the 'A' outcome has relatively more instances compared to the other outcomes.  While this inbalance isn't ideal, I opted to leave the training dataset as-is.

Given that a random forest algorithm will be used, reprocessing (e.g. to standardize) will not make too much of a difference.  Therefore, this step was not performed.

With the help of the *caret* package in R, a **random forest** algorithm (method="rf") was used to create a model to predict the 'classe' outcome in the testing dataset.

```{r chunk6, message=FALSE, cache=TRUE, echo=TRUE}

set.seed(8008)

library(caret)

# Train random forest
fitControl <- trainControl(method="oob")
fit <- train(classe ~ ., data=trainingData, method="rf", ntree=5, trControl=fitControl, do.trace=TRUE)
```

<p/>
The *ntree* parameter was initially set higher, but for the purpose of this exercise (to predict the testing dataset values) an ntree of '5' gives the same answers as higher values of ntree (and it takes much less time to process).

<br/>
<p/>

### Cross-Validation

The 'do.trace=TRUE' parameter to the train/randomForest function shown in the section above outputs the **test set (out of sample) error** after each iteration under the 'OOB' column.  In the next section, I discuss the final out of sample error and the *oob* sampling/cross-validation method that is set in the trainControl parameter.

<br/>
<p/>


## Estimate of the out of sample error

In the section entitled *"The oob error estimate"* at [http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr), Leo Breiman and Adele Cutler indicate that there is no need for (external) cross-validation or a separate test set to get an unbiased estimte of the test set (out of sample) error as it is estimated internally.  As shown in the section above, the *oob* (out-of-bag estimates) method was selected in the **trainControl** function.  This sampling method replaces the need for explicit **cross-validation** when training a prediction model using random forest.

Therefore, the estimate for the out of sample error obtained as:
```{r chunk7,echo=TRUE,message=FALSE}
fit$finalModel$err.rate[[5,1]]
```

That is, the **out of sample error estimate is `r round(fit$finalModel$err.rate[[5,1]] * 100, 3)`%** calculated from the **oob "cross-validation"** (resampling) method used in caret for random forests.


<br/>
<p/>

